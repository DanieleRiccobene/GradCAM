# -*- coding: utf-8 -*-
"""NewGradCam(Last).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DLBpFT20vFDu9Rani3y0zaVtRlzZylhJ
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# ID: scudy247, Token: 717c38d2cbda07836075b014075fef33

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

scudy247_imagenet3_path = kagglehub.dataset_download('scudy247/imagenet3')

print('Data source import complete.')

print(scudy247_imagenet3_path)

import os
import torch
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, Subset, random_split
import torchvision.transforms as transforms

# Define path to the downloaded ImageNet3 dataset
dataset_path = '/root/.cache/kagglehub/datasets/scudy247/imagenet3/versions/1/imagenet3'

# Compose transformation: resize images to 224x224 (ResNet standard) and convert them to PyTorch tensors
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize input images to 224x224 pixels
    transforms.ToTensor()           # Convert PIL image to PyTorch tensor, normalizing pixel values to [0, 1]
])

# Load the full dataset; assumes subfolders are class labels
full_dataset = ImageFolder(root=dataset_path, transform=transform)

# Define split ratio: 80% for training, 20% for validation
train_ratio = 0.8
train_size = int(train_ratio * len(full_dataset))
val_size = len(full_dataset) - train_size  # Compute remaining samples for validation

# Set manual seed for deterministic random split (ensures reproducibility)
torch.manual_seed(42)
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

# Create DataLoader objects for efficient batch loading
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)   # Shuffle training data each epoch
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)      # No shuffle during validation

# Print dataset statistics for confirmation / presentation
print(f"Train dimensions: {len(train_dataset)}")         # Total samples used for training
print(f"Validation dimensions: {len(val_dataset)}")     # Total samples used for validation
print(f"Classes: {full_dataset.classes}")               # List of class names inferred from folder structure

import torch.nn as nn
import torchvision.models as models

# Define a simple CNN model based on ResNet-18 architecture
def create_model(num_classes=3):
    model = models.resnet18(pretrained=False)  # Load ResNet-18 without pre-trained weights
    model.fc = nn.Linear(model.fc.in_features, num_classes)  # Replace final layer for our classification task
    return model

# Use GPU if available, otherwise fallback to CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Instantiate the model and move it to the selected device
model_m1 = create_model(num_classes=3).to(device)

# Print model architecture for confirmation
print(model_m1)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification
optimizer = torch.optim.Adam(model_m1.parameters(), lr=1e-3)  # Adam optimizer with learning rate 0.001

# Cross-entropy measures the difference between the predicted probability distribution and the true label.
# The lower this value, the more confident and accurate the model is.

# Training loop for one epoch
def train(model, dataloader, optimizer, criterion, device):
    model.train()  # Set the model to training mode
    total_loss = 0.0
    for images, labels in dataloader:
        # Move inputs and labels to the selected device (CPU or GPU)
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()             # Reset gradients before backward pass
        outputs = model(images)           # Forward pass
        loss = criterion(outputs, labels) # Compute loss
        loss.backward()                   # Backward pass
        optimizer.step()                  # Update weights

        total_loss += loss.item()         # Accumulate loss value

    avg_loss = total_loss / len(dataloader)  # Compute average loss for the epoch
    return avg_loss

# Evaluation function: computes accuracy on validation data
def evaluate(model, dataloader, device):
    model.eval()  # Set the model to evaluation mode (disables dropout, etc.)
    correct = 0
    total = 0
    with torch.no_grad():  # Disable gradient tracking, useful during inference or validation (when we don't need to update the model parameters)
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)  # Get predicted class index
            total += labels.size(0)                    # Count total samples
            correct += (predicted == labels).sum().item()  # Count correct predictions
    return correct / total  # Return accuracy as a float

# Training loop over 20 epochs
for epoch in range(20):
    train_loss = train(model_m1, train_loader, optimizer, criterion, device)
    val_accuracy = evaluate(model_m1, val_loader, device)
    print(f"Epoch {epoch+1} - Train Loss: {train_loss:.4f} - Val Accuracy: {val_accuracy*100:.2f}%")

!pip install torchcam # To use GradCAM

# Load a pretrained ResNet-50 model with 1000 ImageNet classes
model_resnet50 = models.resnet50(pretrained=True)  # Keep original classification head
model_resnet50 = model_resnet50.to(device)         # Move model to the selected device
model_resnet50.eval()                              # Set to evaluation mode (important for GradCAM)

from torchcam.methods import GradCAM  # Import Grad-CAM method for visualizing model attention

# Initialize GradCAM for both models (our custom ResNet18 and the pretrained ResNet50)
cam_m1 = GradCAM(model_m1, target_layer="layer4")           # For custom model
cam_resnet50 = GradCAM(model_resnet50, target_layer="layer4")  # For pretrained ResNet-50

# Function to generate Grad-CAM heatmaps for a batch of images
def generate_cam_batch(model, cam_extractor, images, device, target_labels=None, use_pred=True):
    model.eval()  # Ensure the model is in evaluation mode
    cams = []     # List to store activation maps

    for i in range(images.size(0)):  # Iterate over batch
        image = images[i].unsqueeze(0).to(device)  # Select one image and add batch dimension
        output = model(image)  # Forward pass (to take the model prediction)

        if use_pred:
            # Use predicted class as the target for GradCAM
            target_class = output.argmax(dim=1).item() # e.g., 'dog'
        else:
            # Use provided ground-truth label if available
            if target_labels is None:
                raise ValueError("target_labels must be provided if use_pred=False")
            target_class = target_labels[i].item()

        # Generate Grad-CAM heatmap for the target class
        activation_map = cam_extractor(target_class, output)
        cams.append(activation_map[0].cpu().detach())  # Store heatmap after detaching from GPU

    return cams  # List of heatmaps, one per input image

!pip install mamba-ssm --no-build-isolation # install mamba

#!pip install git+https://github.com/Arhosseini77/SUM.git # in case the previous cell does not work

!pip install gdown # install gdown to install the model in the Google Drive folder

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/MyDrive/SUM
!gdown 14ma_hLe8DrVNuHCSKoOz41Q-rB1Hbg6A -O net/pre_trained_weights/sum_model.pth # copy the model from Google Drive and save it in the 'net/pre' folder

# Apply Saliency Maps to the images
!python inference_dataset.py \
    --dataset_path /root/.cache/kagglehub/datasets/scudy247/imagenet3/versions/1/imagenet3 \
    --output_path /content/saliency_maps \
    --heat_map_type Overlay

import matplotlib.pyplot as plt
import torchvision.transforms.functional as TF
import numpy as np

# This function displays GradCAM heatmaps and saliency maps for two models
# side by side, using the same input image.
def show_gradcam_saliency(img_tensor, cam1, cam2, sal1, sal2, class_name=""):
    # Convert the input tensor to a PIL image and then to a NumPy array for plotting
    img = TF.to_pil_image(img_tensor.cpu())
    img_np = np.array(img)

    # Convert GradCAM tensors and saliency arrays to NumPy format
    cam1_np = cam1.detach().cpu().numpy()
    cam2_np = cam2.detach().cpu().numpy()
    sal1_np = np.array(sal1)
    sal2_np = np.array(sal2)

    # Create a 2x3 grid for visualization: rows represent models (M1 and ResNet50),
    # columns represent: original image, GradCAM, and Saliency Map
    fig, axs = plt.subplots(2, 3, figsize=(12, 8))
    fig.suptitle(f'Class: {class_name}', fontsize=16)

    # First row: results for model M1
    axs[0, 0].imshow(img_np)
    axs[0, 0].set_title('Original Image')
    axs[0, 1].imshow(img_np)
    axs[0, 1].imshow(cam1_np, cmap='jet', alpha=0.5)
    axs[0, 1].set_title('GradCAM - M1')
    axs[0, 2].imshow(sal1_np)
    axs[0, 2].set_title('Saliency Map')

    # Second row: results for pretrained ResNet50
    axs[1, 0].imshow(img_np)
    axs[1, 0].set_title('Original Image')
    axs[1, 1].imshow(img_np)
    axs[1, 1].imshow(cam2_np, cmap='jet', alpha=0.5)
    axs[1, 1].set_title('GradCAM - R50')
    axs[1, 2].imshow(sal2_np)
    axs[1, 2].set_title('Saliency Map')

    # Remove axis ticks for a cleaner look
    for ax in axs.flat:
        ax.axis('off')

    # Adjust layout and display the figure
    plt.tight_layout()
    plt.show()

import os
from torchvision import transforms
from PIL import Image
import torch.nn.functional as F
import torch

# This function thresholds a GradCAM heatmap to produce a binary activation map
# Pixels with intensity greater than (threshold * the maximum value) are set to 1, the others to 0
def binarize_cam(cam, threshold=0.3):
    return (cam > threshold).float()

# This function computes the Intersection over Union (IoU) between two binary maps
# It's used to measure the overlap between the GradCAM and Saliency Maps
def iou(cam1, cam2):
    intersection = torch.logical_and(cam1.bool(), cam2.bool()).sum().float()
    union = torch.logical_or(cam1.bool(), cam2.bool()).sum().float()
    if union == 0:
        return 0.0
    return (intersection / union).item()

# Define the path where the external saliency maps are stored
saliency_path = '/content/saliency_maps'

# Same standard preprocessing transform and tensor conversion
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

# This dictionary will store the saliency map paths for each image,
# using the base filename (without "_overlay.png") as the key
saliency_dict = {}
for class_folder in os.listdir(saliency_path): # Loop over each class folder inside the saliency_maps directory (each folder corresponds to a different class label)
    class_path = os.path.join(saliency_path, class_folder)
    for fname in os.listdir(class_path):
        if "_overlay" in fname: # Only filter those files that contain '_overlay' in the filename (SUM creates two versions of the Saliency Maps)
            base = fname.replace("_overlay.png", "")
            saliency_dict[base] = os.path.join(class_path, fname)

# Lists to store the IoU values for both models
iousm1 = []
iousr50 = []

# Loop through the validation dataset in batches
for batch_idx, (images, labels) in enumerate(val_loader):
    images = images.to(device)
    labels = labels.to(device)

    # Generate GradCAM maps for the current batch from both models (M1 and ResNet50)
    cams_m1 = generate_cam_batch(model_m1, cam_m1, images, device, target_labels=labels, use_pred=False)
    cams_r50 = generate_cam_batch(model_resnet50, cam_resnet50, images, device, target_labels=labels, use_pred=False)

    # Get the original dataset indices of the current batch
    batch_indices = val_dataset.indices[batch_idx * val_loader.batch_size : (batch_idx + 1) * val_loader.batch_size]

    # Iterate through the batch one image at a time
    for i in range(len(images)):
        idx = batch_indices[i]
        img_path = full_dataset.samples[idx][0]
        img_name = os.path.splitext(os.path.basename(img_path))[0]

        # Skip images for which are not in the dictionary previously created
        if img_name not in saliency_dict:
            continue

        # Load the external saliency map (from SUM) and binarize it
        sal_img = Image.open(saliency_dict[img_name])
        sal_tensor = transform(sal_img).to(device).squeeze()
        sal_bin = binarize_cam(sal_tensor).to(device)

        # Resize the GradCAM outputs to match the saliency map resolution (224x224)
        cam1_resized = F.interpolate(cams_m1[i].unsqueeze(0), size=(224,224), mode='bilinear', align_corners=False).squeeze()
        cam2_resized = F.interpolate(cams_r50[i].unsqueeze(0), size=(224,224), mode='bilinear', align_corners=False).squeeze()

        # Binarize the GradCAM maps for comparison
        cam1_bin = binarize_cam(cam1_resized).to(device)
        cam2_bin = binarize_cam(cam2_resized).to(device)

        # Compute IoU between GradCAM maps and external saliency map, and store results
        iousm1.append(iou(cam1_bin, sal_bin))
        iousr50.append(iou(cam2_bin, sal_bin))

        # For the first 10 examples, show visual comparison
        if len(iousm1) <= 10:
            img = images[i]
            lbl = labels[i]
            class_name = full_dataset.classes[lbl.item()]
            sal_img1 = Image.open(saliency_dict[img_name])

            show_gradcam_saliency(
                img_tensor=img,
                cam1=cam1_resized,
                cam2=cam2_resized,
                sal1=sal_img1,
                sal2=sal_img1,
                class_name=class_name
            )

# Compute and print the average IoU for both models over the whole validation set
avg_iou_m1 = sum(iousm1) / len(iousm1)
avg_iou_r50 = sum(iousr50) / len(iousr50)
print(f"Average IoU M1 vs SUM: {avg_iou_m1:.4f}")
print(f"Average IoU ResNet50 vs SUM: {avg_iou_r50:.4f}")